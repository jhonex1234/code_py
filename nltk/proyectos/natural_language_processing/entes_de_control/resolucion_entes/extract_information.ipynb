{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.corpora import Dictionary\n",
    "from glob import glob\n",
    "import nltk \n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from time import time\n",
    "from unicodedata import normalize\n",
    "#import database as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[20178200393205, ', Página, 2, de, 4, Las, pru...</td>\n",
       "      <td>11122017132607 confirma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Q, íº, Superservícío, 5, ``, TODOS, mau, ., g...</td>\n",
       "      <td>11122017133429 confirma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Superintendencia, de, Servicios, NUEVO, PA'S,...</td>\n",
       "      <td>11122017133637 confirma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[—-—-—, ronesronuu, ., :, MDNP, NUEVO, PAIS, e...</td>\n",
       "      <td>11122017133150 confirma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Página, 2, de, 4, 20178200393215, Las, prueba...</td>\n",
       "      <td>11122017132933 confirma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                     Name\n",
       "0  [20178200393205, ', Página, 2, de, 4, Las, pru...  11122017132607 confirma\n",
       "1  [Q, íº, Superservícío, 5, ``, TODOS, mau, ., g...  11122017133429 confirma\n",
       "2  [Superintendencia, de, Servicios, NUEVO, PA'S,...  11122017133637 confirma\n",
       "3  [—-—-—, ronesronuu, ., :, MDNP, NUEVO, PAIS, e...  11122017133150 confirma\n",
       "4  [Página, 2, de, 4, 20178200393215, Las, prueba...  11122017132933 confirma"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ruta\n",
    "#filelist = glob('/home/jhon/proyectos/natural_language_processing/entes_de_control/resolucion_entes/entes_de_control_test/part1/*.txt')\n",
    "filelist = glob('/home/jhonex/document_e/*.txt')\n",
    "#lista \n",
    "letters = []\n",
    "for file in filelist:\n",
    "    #\n",
    "    with open(file, 'r') as txtfile: \n",
    "        letters.append((tokenize.word_tokenize(txtfile.read()), re.match('.*/(.*)-.*', file).group(1)))\n",
    "letters = pd.DataFrame(letters, columns=['Text', 'Name'])\n",
    "letters = pd.DataFrame([(np.concatenate(letters[letters.Name == name].Text.values), name) \n",
    "                        for name in letters.Name.unique()], columns=['Text', 'Name'])\n",
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDocs(letters):\n",
    "    start = time()\n",
    "    clean_letters = []\n",
    "    for _, letter in letters.iterrows():\n",
    "        words = [normalize('NFKD', word.lower()).encode('ascii', 'ignore').decode('utf-8') for word in letter['Text']]\n",
    "        # is alphabetic character, not is a stopword, not is a custom word, has at least three letters, \n",
    "        # has at least one vowel, has at least one consonant\n",
    "        words = [word for word in words if word not in stopwords.words('spanish')]\n",
    "        clean_letters.append(' '.join(words))\n",
    "\n",
    "    print('Cleaning took %.2f seconds' % (time() - start))\n",
    "    return pd.DataFrame(clean_letters)\n",
    "\n",
    "dup = letters.insert(1, 'CleanText', cleanDocs(letters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "letters.CleanText[letters.Name == 'RESOLUCIрN APELACIрN'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def context(target_word, tar_passage, left_margin = 10, right_margin = 10):\n",
    "    tokens = tokenize.word_tokenize(tar_passage)\n",
    "    text = nltk.Text(tokens)\n",
    "    c = nltk.ConcordanceIndex(text.tokens, key = lambda s: s.lower())\n",
    "    concordance_txt = [text.tokens[offset - left_margin : offset + right_margin]\n",
    "                       for offset in c.offsets(target_word)]\n",
    "    return [''.join([x + ' ' for x in con_sub]) for con_sub in concordance_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText = re.sub('[^\\w\\s]', '', letters[letters.Name == 'RESOLUCIрN APELACIрN'].CleanText.values[0])\n",
    "val1=re.findall('(\\d{16})', cleanText)\n",
    "val2=re.findall('sspd\\s*(\\d{14})', cleanText)\n",
    "\n",
    "numero=set(re.findall('\\s(\\d{14})\\s', cleanText))\n",
    "val3=set(numero)-set(val2)\n",
    "\n",
    "#print(set(re.findall('re\\s*(\\d{13})', cleanText)))\n",
    "print(val1)\n",
    "print(val2)\n",
    "print(val3)\n",
    "resoluciones = ['requerir', 'apertura de investigacion', 'sancion', 'archivar', 'confirmar', 'revocar', \n",
    "                'rechazar', 'tramitar', 'improcedente', 'rechazo', 'procedente', 'modificar', 'inhibirse', \n",
    "                'no acceder', 'corregir', 'aclarar']\n",
    "for resolucion in resoluciones:\n",
    "    coincidence = re.findall(resolucion + ' ', cleanText)\n",
    "    if coincidence:\n",
    "        print(coincidence)\n",
    "        \n",
    "results = context('modificar', cleanText, 0, 10)\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "#print(re.fidall('sancion', cleanText))\n",
    "if results == [] :\n",
    "            print('\\n:(\\n\"no se se detecto contexto\" ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([7,8,9,0,6,7,8,5])-set([5,3,7,9,2,3,4,2,3])\n",
    "le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dic = Dictionary(letters.CleanText.apply(lambda text: tokenize.word_tokenize(text)).values)\n",
    "#dic.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test = letters[letters.index == 8].CleanText.apply(lambda text: tokenize.word_tokenize(text)).values[0]\n",
    "#dic.doc2bow(['servicios', 'nuevo', 'pais', 'publicos', 'domiciliarios'])\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(one_hot_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleanText = re.sub('[^\\w\\s]', '', letters[letters.Name == 'userFile1500499850858'].CleanText.values[0])\n",
    "val1=re.findall('(\\d{16})', cleanText)\n",
    "val2=re.findall('sspd\\s*(\\d{14})', cleanText)\n",
    "\n",
    "numero=set(re.findall('\\s(\\d{14})\\s', cleanText))\n",
    "val3=set(numero)-set(val2)\n",
    "\n",
    "#print(set(re.findall('re\\s*(\\d{13})', cleanText)))\n",
    "print(val1)\n",
    "print(val2)\n",
    "print(val3)\n",
    "\n",
    "\n",
    "#////////////////no tocar/////////////////////////////////////7\n",
    "resoluciones = ['requerir', 'apertura de investigacion', 'sancion', 'archivar', 'confirmar', 'revocar', \n",
    "                'rechazar', 'tramitar', 'improcedente', 'rechazo', 'procedente', 'modificar', 'inhibirse', \n",
    "                'no acceder', 'corregir', 'aclarar']\n",
    "for resolucion in resoluciones:\n",
    "    coincidence = re.findall(resolucion + ' ', cleanText)\n",
    "    if coincidence:\n",
    "        print(coincidence)\n",
    "        \n",
    "results = context('modificar', cleanText, 0, 10)\n",
    "\n",
    "#////////////////no tocar/////////////////////////////////////7\n",
    "\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "#print(re.fidall('sancion', cleanText))\n",
    "if results == [] :\n",
    "            print('\\n:(\\n\"no se se detecto contexto\" ') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
